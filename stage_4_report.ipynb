{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('rabbit': venv)"
  },
  "interpreter": {
   "hash": "3eff666360e6dfa62dcef3671dc59bfca7f64d1a166f3a559c38a297c135904c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 深層学習day3\n",
    "## Section1：再帰型ニューラルネットワークの概念\n",
    "## Section2：LSTM\n",
    "## Section3：GRU\n",
    "## Section4：双方向RNN\n",
    "## Section5：Seq2Seq\n",
    "## Section6：Word2vec\n",
    "## Section7：Attention Mechanism\n",
    "# 深層学習day4\n",
    "## Section1：強化学習\n",
    "## Section2：AlphaGo\n",
    "## Section3：軽量化・高速化技術\n",
    "## Section4：応用モデル\n",
    "## Section5：Transformer\n",
    "## Section6：物体検知・セグメンテーション"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 深層学習day3\n",
    "## Section1：再帰型ニューラルネットワークの概念"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- 時系列データ\n",
    "<br>\n",
    "時間的なつながりを持つデータ\n",
    "- RNNの全体像\n",
    "<br>\n",
    "時系列データを扱うためのニューラルネットワーク\n",
    "<br>\n",
    "ネットワーク中に再帰的構造を持つことで、過去の入力情報を保持する\n",
    "- BPTT\n",
    "<br>\n",
    "RNNにおける誤差逆伝搬の方法\n",
    "<br>\n",
    "RNNが再帰的な構造を含むため、過去の状態を考慮した更新式になる\n",
    "<br>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 実装演習\n",
    "<br>\n",
    "バイナリ加算を例としてRNNの学習を確認した\n",
    "<br>\n",
    "またハイパーパラメータによる違いを確認した\n",
    "- 重みの初期値の分散\n",
    "<br>\n",
    "大きすぎても小さすぎても収束しない\n",
    "- 学習率\n",
    "<br>\n",
    "大きいと学習が高速になるが、不安定にもなる\n",
    "- 隠れ層のサイズ\n",
    "<br>\n",
    "大きすぎても小さすぎても問題\n",
    "- 初期値\n",
    "<br>\n",
    "Xavierでは学習が不安定\n",
    "<br>\n",
    "Heではうまくいく\n",
    "- 活性化関数\n",
    "<br>\n",
    "ReLUではうまく学習しない\n",
    "<br>\n",
    "ReLU+Xavier、ReLU+Heではうまくいく\n",
    "<br>\n",
    "中間層がtanhの場合、どの初期値でもうまくいく"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section2：LSTM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- RNNの課題\n",
    "<br>\n",
    "時系列を遡るほど、勾配が消失または爆発する\n",
    "<br>\n",
    "そのため、長い時系列の学習が困難\n",
    "- LSTM\n",
    "<br>\n",
    "RNNの一種\n",
    "<br>\n",
    "入出力の制御を行うために特徴的な構造を持つ\n",
    "  - CEC\n",
    "  <br>\n",
    "  過去の入力に関する情報を保持するための構造\n",
    "  <br>\n",
    "  勾配消失・爆発を避けるために勾配を1にする\n",
    "  - 入力ゲート\n",
    "  <br>\n",
    "  入力をCECにどれだけ反映させるかを制御する\n",
    "  - 出力ゲート\n",
    "  <br>\n",
    "  CECの情報をどれだけ出力に反映させるかを制御する\n",
    "  - 忘却ゲート\n",
    "  <br>\n",
    "  CECが持つ過去の情報をどれだけ忘却させるかを制御する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "<br>\n",
    "近年の研究では、時系列データ、特に言語データに対しては\n",
    "<br>\n",
    "Attentionがよく用いられる印象がある。\n",
    "<br>\n",
    "しかしAttentionは必要なリソースが大きいため、\n",
    "<br>\n",
    "まずRNNを利用するときのデファクトスタンダードとして、\n",
    "<br>\n",
    "LSTMが利用されているように思う。\n",
    "<br>\n",
    "実際に使う機会もあると考えられるため、理解したい。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section3：GRU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- LSTMの課題\n",
    "<br>\n",
    "パラメータ数が多く、計算負荷が大きい\n",
    "- GRU\n",
    "<br>\n",
    "LSTMに比べてパラメータ数が少なく、かつ精度が高い\n",
    "<br>\n",
    "LSTMに比べてゲートの数を減らすことで効率化\n",
    "  - リセットゲート\n",
    "  <br>\n",
    "  隠れ層の状態をどのように保持するかを制御する\n",
    "  - 更新ゲート\n",
    "  <br>\n",
    "  現在の隠れ層の状態と過去の状態をどのようにシャッフルするかを制御する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "<br>\n",
    "LSTMと同じように利用できるものと考えられるが、\n",
    "<br>\n",
    "LSTMとの違いをよく理解する必要がある。\n",
    "<br>\n",
    "パラメータが少なく、利用しやすいと考えられるため、\n",
    "<br>\n",
    "違いを理解して説明できるようにしたい。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section4：双方向RNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- 双方向RNN\n",
    "<br>\n",
    "過去の情報だけでなく、未来の情報も利用することで精度を向上させる\n",
    "<br>\n",
    "文章の推敲や機械翻訳等に応用される\n",
    "<br>\n",
    "通常の向き（過去から未来）のRNNと逆向き（未来から過去）の2つのRNNを組み合わせたような構造を持つ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section5：Seq2Seq"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- Seq2Seq\n",
    "<br>\n",
    "2つのニューラルネットワークを組み合わせた構造を持つ\n",
    "<br>\n",
    "前段のニューラルネットワーク（Encoder）で入力の情報をまとめる\n",
    "<br>\n",
    "後段のニューラルネットワーク（Decoder）でまとめた情報から出力を生成する\n",
    "  - Encoder RNN\n",
    "  <br>\n",
    "  入力ベクトルを順にRNNに入力し、最終的な隠れ層の状態（thought vector）を保持する\n",
    "  - Decoder RNN\n",
    "  <br>\n",
    "  アウトプットデータをトークン（単語等）ごとに生成する\n",
    "  <br>\n",
    "  thought vectorを元に出力を生成する\n",
    "  <br>\n",
    "  生成した出力は次の時刻のDecoder RNNの入力としても利用する\n",
    "- HRED\n",
    "<br>\n",
    "Seq2SeqのEncoderがまとめた過去の情報をRNNで利用する\n",
    "<br>\n",
    "Seq2Seqと違い、文脈の情報を考慮可能\n",
    "- VHRED\n",
    "<br>\n",
    "HRED + VAE\n",
    "<br>\n",
    "HREDに比べて多様性のある返答が可能"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "画像データに対してはCNNの利用がかなり一般的になってきた。\n",
    "<br>\n",
    "同様に言語データに対してのRNNの利用も今後普及してくるものと予想される。\n",
    "<br>\n",
    "その中でもSeq2SeqのようなEncoder-Decoderモデルは重要度が高いと考えられる。\n",
    "<br>\n",
    "よく理解したい。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section6：Word2vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- Word2vec\n",
    "<br>\n",
    "単語のような可変長文字列をRNNに入力できるように、固定長ベクトルに変換する\n",
    "<br>\n",
    "大規模データの分散表現の学習を、現実的な計算速度・メモリ量で可能\n",
    "- Word2vecとone-hotベクトル\n",
    "  - Word2vec\n",
    "  <br>\n",
    "  ボキャブラリ数ではなく、任意の次元の固定長ベクトルで単語を表す\n",
    "  - one-hotベクトル\n",
    "  <br>\n",
    "  ボキャブラリ数の固定長ベクトルで単語を表す"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "<br>\n",
    "言語データに対する深層学習では、Transformerなど\n",
    "<br>\n",
    "大きなリソースを要するものが多い。\n",
    "<br>\n",
    "その意味で、必要なメモリを減らせるWord2vecは有用である。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section7：Attention Mechanism"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- Seq2Seqの課題\n",
    "<br>\n",
    "Seq2Seqは長い文章の対応が難しい\n",
    "<br>\n",
    "単語数によらず、固定次元ベクトルに入力をまとめる必要がある\n",
    "- Attention Mechanism\n",
    "<br>\n",
    "文章の長さに応じて、内部表現の次元が大きくなる\n",
    "<br>\n",
    "入力と出力がどの程度関連しているかを学習する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "<br>\n",
    "近年の研究では言語だけでなく、画像などのデータに対しても\n",
    "<br>\n",
    "Attentionを利用したものが多くなっている。\n",
    "<br>\n",
    "今後の普及が予想される。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 深層学習Day4\n",
    "## Section1：強化学習"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- 強化学習\n",
    "<br>\n",
    "長期的に報酬を最大化できるように環境の中で行動を選択できるエージェントを作る\n",
    "<br>\n",
    "マーケティング（顧客ごとのメール送信）などに応用可能\n",
    "- 概要\n",
    "  1. エージェントは状態を観測して、方策を元に行動する\n",
    "  1. 行動の結果として報酬が得られる\n",
    "  1. 得られた報酬から方策を修正する\n",
    "- 探索と利用のトレードオフ\n",
    "<br>\n",
    "不完全な知識をもとに行動を探索する必要\n",
    "<br>\n",
    "過去の知識からベストな行動をとるか、未知の行動を探索するかのトレードオフがある\n",
    "- 方策関数\n",
    "  <br>\n",
    "  ある状態でどのような行動をとるかの確率を与える関数\n",
    "- 価値関数\n",
    "<br>\n",
    "ゴールまで今の方策を続けたときの報酬の予測値（価値）を表す\n",
    "<br>\n",
    "状態価値関数・行動価値関数の2つがある\n",
    "  - 状態価値関数\n",
    "  <br>\n",
    "  ある状態の価値を表す\n",
    "  - 行動価値関数\n",
    "  <br>\n",
    "  ある状態・行動の価値を表す\n",
    "- 方策勾配法\n",
    "<br>\n",
    "方策をモデル化して、方策の良さの勾配に応じて最適化する手法\n",
    "<br>\n",
    "方策の良さを定義する必要"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "<br>\n",
    "分類などのタスクに比べて、強化学習は応用が進んでいない印象はあるが、\n",
    "<br>\n",
    "強化学習なら解決可能なタスクも多い。\n",
    "<br>\n",
    "利用可能な計算資源の増加に伴い、普及が進む可能性がある。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section2：AlphaGo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- AlphaGo Lee\n",
    "<br>\n",
    "方策関数、価値関数をそれぞれニューラルネットワークで表す\n",
    "<br>\n",
    "ニューラルネットワークの入力には石の配置だけでなく、取れる石の数などを含む\n",
    "- Alpha Goの学習\n",
    "  1. 教師あり学習によるRollOutPolicyとRollOutValueの学習\n",
    "  1. 強化学習によるPolicyNetの学習\n",
    "  1. 強化学習によるValueNetの学習\n",
    "- alphaGo Zero\n",
    "  - AlphaGo Leeとの違い\n",
    "    1. 教師あり学習を行わず、強化学習のみで作成\n",
    "    1. 入力からヒューリスティックな要素を排除し、石の配置のみにした\n",
    "    1. PolicyNetとValueNetを統合\n",
    "    1. Residual Net（ネットワーク中のショートカット）を導入\n",
    "    1. RollOutシミュレーションをなくした"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "Alpha GoとDQNは深層強化学習のエポックメイキング的なモデルであり、\n",
    "<br>\n",
    "発表当時は大きな話題となった。\n",
    "<br>\n",
    "画像分類などと比べると派手なこともあるが、\n",
    "<br>\n",
    "強化学習に対する期待もあると考えられる。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section3：軽量化・高速化技術"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "深層学習は多くのリソースを要するため、効率的な計算が重要\n",
    "- データ並列化\n",
    "<br>\n",
    "親モデルを各ワーカにコピー\n",
    "<br>\n",
    "データを分割し、各ワーカで計算\n",
    "<br>\n",
    "各ワーカの学習結果の統合方法に同期型・非同期型がある\n",
    "<br>\n",
    "非同期型のほうが速いが、学習が不安定\n",
    "- モデル並列化\n",
    "<br>\n",
    "親モデルを各ワーカに分割し、それぞれ学習\n",
    "- 量子化\n",
    "<br>\n",
    "パラメータの精度を落とす（64bitから32bitなど）ことでメモリと演算処理を削減\n",
    "<br>\n",
    "パラメータの精度を落とすとモデルの表現力が下がるため、極端に精度を落とさない\n",
    "- 蒸留\n",
    "<br>\n",
    "精度の高い複雑なモデル（教師モデル）で学習を行った結果を、軽量なモデル（生徒モデル）に継承させる\n",
    "- プルーニング\n",
    "<br>\n",
    "寄与の少ないニューロンの削減を行い、モデルを圧縮"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "<br>\n",
    "深層学習の実用面での問題点としては、\n",
    "<br>\n",
    "必要なデータが多いことと必要な計算資源が多いことが挙げられる。\n",
    "<br>\n",
    "学習時だけでなく、予測時にも計算資源が問題となることは多い。\n",
    "<br>\n",
    "後者に対して、軽量化・高速化技術は実用上重要な技術と考えられる。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section4：応用モデル"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- MobileNet\n",
    "<br>\n",
    "Depthwise Separable Convolutionで計算量を削減\n",
    "<br>\n",
    "具体的にはDepthwise ConvolutionとPointwise Convolutionの組み合わせ\n",
    "  - Depthwise Convolution\n",
    "  <br>\n",
    "  チャネルごとに共通のフィルタで畳み込み\n",
    "  - Pointwise Convolution\n",
    "  <br>\n",
    "  入力マップのポイントごとに畳み込み\n",
    "  <br>\n",
    "  1 x 1 Convとも呼ばれる\n",
    "- DenseNet\n",
    "<br>\n",
    "Dense Blockを利用して、深い層でも学習可能\n",
    "  - Dense Block\n",
    "  <br>\n",
    "  畳み込みで計算した特徴マップを足し合わせていく\n",
    "  <br>\n",
    "  足し合わせるサイズがgrowth rateで制御\n",
    "  - Transition Layer\n",
    "  <br>\n",
    "  特徴マップのサイズを変更し、ダウンサンプリングを行う\n",
    "- 正規化\n",
    "  - Batch Norm\n",
    "  <br>\n",
    "  ミニバッチに含まれるsampleの同一チャネルを正規化\n",
    "  <br>\n",
    "  バッチサイズが小さいと問題\n",
    "  - Layer Norm\n",
    "  <br>\n",
    "  それぞれのsampleのすべてのpixelを正規化\n",
    "  <br>\n",
    "  入力データのスケール、重み行列のスケール・シフトに対してロバスト\n",
    "  - Instance Norm\n",
    "  <br>\n",
    "  Layer Normに加えてチャネルも正規化\n",
    "  <br>\n",
    "  コントラストの正規化に寄与\n",
    "  <br>\n",
    "  画像のスタイル転送やテクスチャ合成で利用\n",
    "- Wavenet\n",
    "<br>\n",
    "時系列データに対して畳み込みを行う\n",
    "  - Dilated convolution\n",
    "  <br>\n",
    "  層が深くなるにつれて畳み込むリンクを離す\n",
    "  <br>\n",
    "  受容野を簡単に増加可能"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "<br>\n",
    "深層学習は発展途上の分野であり、次々と新しいモデルが発表される。\n",
    "<br>\n",
    "代表的なモデルだけでも追っていく必要があるだろう。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section5：Transformer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- 言語モデル\n",
    "<br>\n",
    "時刻t-1までの情報から次に来る単語の事後確率を計算\n",
    "<br>\n",
    "RNNで事後確率を計算できれば、文章生成も可能\n",
    "- Seq2Seq\n",
    "<br>\n",
    "Encoderで計算した内部状態ベクトルをDecoderに渡す\n",
    "- Attention\n",
    "<br>\n",
    "入力と出力の対応関係（注意）を分配\n",
    "- Transformer\n",
    "<br>\n",
    "RNNを利用せず、Attentionだけで構成\n",
    "<br>\n",
    "EncoderとDecoderの両方でAttentionを利用"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 実装演習\n",
    "<br>\n",
    "Transformerによる翻訳を行った。\n",
    "<br>\n",
    "訓練データのサイズ、学習のEpoch共に少ないが、\n",
    "<br>\n",
    "多少それらしい翻訳結果が出力された。\n",
    "<br>\n",
    "一方で1 Epochの学習にはかなり時間がかかった。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section6：物体検知・セグメンテーション"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- 物体認識タスクの種類\n",
    "  - 分類\n",
    "  <br>\n",
    "  画像に対してラベルを出力\n",
    "  - 物体検知\n",
    "  <br>\n",
    "  bounding boxを出力\n",
    "  - 意味領域分割\n",
    "  <br>\n",
    "  ピクセルに対してラベルを出力\n",
    "  <br>\n",
    "  インスタンスを区別しない\n",
    "  - 個体領域分割\n",
    "  <br>\n",
    "  ピクセルに対してラベルを出力\n",
    "  <br>\n",
    "  インスタンスを区別\n",
    "- 物体検知のフレームワーク\n",
    "  - 2段階検出器\n",
    "  <br>\n",
    "  精度は高いが計算量が大きい\n",
    "  <br>\n",
    "  SSD、YOLOなど\n",
    "  - 1段階検出器\n",
    "  <br>\n",
    "  精度が低いが計算量が小さい\n",
    "  <br>\n",
    "  RCNN、FPNなど\n",
    "- SSD\n",
    "<br>\n",
    "物体検知に利用\n",
    "<br>\n",
    "  - 出力\n",
    "  <br>\n",
    "  特徴マップの各特徴量に対してk個のDefault Boxを出力\n",
    "  <br>\n",
    "  1つのDefault Boxに対して、クラス・位置・サイズを出力\n",
    "  - Non-Maximum Suppression\n",
    "  <br>\n",
    "  重なったBounding Boxを制約する\n",
    "  - Hard Negative Mining\n",
    "  <br>\n",
    "  物体に対して背景が多いことに対処\n",
    "- Semantic Segmentation\n",
    "  - Deconvolution/Transposed convolution\n",
    "  <br>\n",
    "  up samplingを行う\n",
    "  <br>\n",
    "  畳み込み同様にkernel size、padding、strideを指定\n",
    "  <br>\n",
    "  poolingで失った情報を復元はできない\n",
    "  - U-Net\n",
    "  <br>\n",
    "  EncoderとDecoderで構成\n",
    "  <br>\n",
    "  EncoderからDecoderへのSkip-connectionでup sampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "<br>\n",
    "実用上、画像分類よりも物体検知やセグメンテーションができることが\n",
    "<br>\n",
    "好ましい問題が多い。\n",
    "<br>\n",
    "よく理解し、利用可能なスキルとして獲得する必要がある。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}