{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('rabbit': venv)"
  },
  "interpreter": {
   "hash": "3eff666360e6dfa62dcef3671dc59bfca7f64d1a166f3a559c38a297c135904c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 深層学習day1\n",
    "## Section1：入力層～中間層\n",
    "## Section2：活性化関数\n",
    "## Section3：出力層\n",
    "## Section4：勾配降下法\n",
    "## Section5：誤差逆伝搬法\n",
    "# 深層学習day2\n",
    "## Section1：勾配消失問題\n",
    "## Section2：学習率最適化手法\n",
    "## Section3：過学習\n",
    "## Section4：畳み込みニューラルネットワークの概念\n",
    "## Section5：最新のCNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 深層学習day1\n",
    "## Section1：入力層～中間層"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- ニューラルネットワークの全体像\n",
    "<br>\n",
    "ニューラルネットワークは入力層・中間層・出力層の3つから構成\n",
    "<br>\n",
    "各層が重みとバイアスを介してつながっている\n",
    "<br>\n",
    "重みとバイアスの値を調節するのが、ニューラルネットワークの学習\n",
    "- 入力層～中間層\n",
    "<br>\n",
    "入力層で入力された値に対して、その線形結合+バイアスを中間層に渡す\n",
    "<br>\n",
    "中間層は渡された値を活性化関数に通して出力する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 実装演習\n",
    "実装演習では入力層～中間層のPython実装を確認した\n",
    "<br>\n",
    "numpyの機能を利用すると簡易な記述が可能\n",
    " - np.array：ベクトル・行列を定義\n",
    " - np.zeros：ベクトル・行列を0で初期化\n",
    " - np.random.rand：0～1の乱数で初期化\n",
    " - np.dot：ベクトル・行列の積"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "入力層～中間層までの部分では線形回帰モデルなどと大差ないため、理解が容易\n",
    "<br>\n",
    "以下の要素を加えることで、ニューラルネットワークが多様な表現力を持つと考えられる\n",
    " - 活性化関数\n",
    " - 中間層が複数のユニットから構成\n",
    " - 出力層を介する"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section2：活性化関数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "ニューラルネットワークでは活性関数を利用することで、入力の非線形変換を行う\n",
    "中間層出力の重みづけに貢献\n",
    "<br>\n",
    "- 中間層で利用される活性化関数\n",
    "  - ステップ関数\n",
    "  <br>\n",
    "  閾値を超えるかどうかを判定し0か1を返す\n",
    "  <br>\n",
    "  線形分離可能なものしか学習できず、現在では利用されない\n",
    "  - シグモイド（ロジスティック）関数\n",
    "  <br>\n",
    "  0～1の間を緩やかに変化する関数\n",
    "  <br>\n",
    "  ニューラルネットワーク普及のきっかけとなったが、勾配消失問題がある\n",
    "  - RELU関数\n",
    "  <br>\n",
    "  入力が0以下ならば0、0以上ならば入力をそのまま返す\n",
    "  <br>\n",
    "  勾配消失問題の回避とニューラルネットワークのスパース化に貢献\n",
    "- 出力層で利用される活性化関数\n",
    "  - ソフトマックス関数\n",
    "  - 恒等写像\n",
    "  - シグモイド関数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 実装演習\n",
    "活性化関数のPython実装を確認\n",
    "<br>\n",
    "ステップ関数・シグモイド関数・RELU関数のいずれも単純であり、1行で実装可能\n",
    "- ステップ関数：np.where（場合分け）を利用\n",
    "- シグモイド関数：np.exp（指数関数）を利用\n",
    "- RELU関数：np.maximum（最大値）を利用"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "深層学習ではRELU関数が標準的に利用される。\n",
    "<br>\n",
    "それまでよく利用されていたシグモイド関数では微分したときに値の最大値が1/4となるため、\n",
    "<br>\n",
    "勾配消失問題が発生して多層の学習が困難であった。\n",
    "<br>\n",
    "RELU関数では微分した時の最大値が1であるため、\n",
    "<br>\n",
    "勾配消失問題を回避可能。\n",
    "<br>\n",
    "しかし、近年ではRELU関数に対してDying RELU問題という別の問題が報告されており、\n",
    "<br>\n",
    "leaky RELUなどの派生が提案されている。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section3：出力層"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- 出力層の役割\n",
    "<br>\n",
    "中間層までの出力を人が欲しい形に変形\n",
    "<br>\n",
    "構成方法は中間層と大差なし\n",
    "- 誤差関数\n",
    "<br>\n",
    "ニューラルネットワークの出力と教師データがどの程度合っているかを表現\n",
    "  - 2乗誤差：回帰に利用\n",
    "  - 交差エントロピー誤差：分類に利用\n",
    "<br>\n",
    "誤差を微分してニューラルネットワークの学習に利用\n",
    "- 出力層の活性化関数\n",
    "<br>\n",
    "出力層と中間層では利用される活性化関数が異なる\n",
    "  - 中間層では入力の強弱を調整するが、出力層では強弱はそのまま\n",
    "  - 確率を出力する場合には値を制限\n",
    "具体的に利用される活性化関数は以下がある\n",
    "  - 恒等写像\n",
    "  <br>\n",
    "  入力をそのまま返す関数\n",
    "  回帰に利用\n",
    "  - シグモイド関数\n",
    "  <br>\n",
    "  二値分類に利用\n",
    "  - ソフトマックス関数\n",
    "  <br>\n",
    "  他クラス分類に利用"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 実装演習\n",
    "誤差関数や出力層の活性化関数を確認した\n",
    "<br>\n",
    "ソフトマックス関数は本質的には非常に単純な形で実装可能だが、\n",
    "<br>\n",
    "実際の利用時の問題対策のためにやや複雑となっていた\n",
    "- ミニバッチ\n",
    "- オーバーフロー対策\n",
    "<br>\n",
    "誤差関数においても、交差エントロピー関数は本質的には単純だが、\n",
    "<br>\n",
    "いくつかの問題対策が入っていた\n",
    "- ミニバッチ\n",
    "- one-hot-vectorの逆変換\n",
    "- log(0)の対策"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "深層学習でよく利用されるパッケージの1つであるKerasには\n",
    "<br>\n",
    "様々な誤差関数が定義されている。\n",
    "<br>\n",
    "以下はその一部である。\n",
    "<br>\n",
    "実際の問題に深層学習を適用する際には、問題の性質に応じて使い分けが必要かもしれない。\n",
    "- mean_squared_error\n",
    "<br>\n",
    "2乗誤差\n",
    "- mean_absolute_error\n",
    "<br>\n",
    "2乗ではなく、絶対値を利用\n",
    "- mean_absolute_percentage_error\n",
    "<br>\n",
    "教師データに対する絶対誤差の比率を利用\n",
    "- mean_squared_logarithmic_error\n",
    "<br>\n",
    "ネットワーク出力と教師データにlogを介した後に2乗誤差"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section4：勾配降下法"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- ニューラルネットワークや深層学習の学習\n",
    "<br>\n",
    "誤差を最小化する重みやバイアスを最小化する\n",
    "<br>\n",
    "重みやバイアスの最適化のために勾配降下法を利用\n",
    "- 勾配降下法\n",
    "<br>\n",
    "重みの微分に学習率をかけて、重みを更新する\n",
    "<br>\n",
    "適切な学習率を設定する必要がある\n",
    "  - 学習率が大きい場合\n",
    "  <br>\n",
    "  収束しない場合がある\n",
    "  <br>\n",
    "  学習にかかる時間は少ない\n",
    "  - 学習率が大きい場合\n",
    "  <br>\n",
    "  学習に時間がかかる\n",
    "  <br>\n",
    "  局所解に収束しやすい\n",
    "- 勾配降下法のためのアルゴリズム\n",
    "<br>\n",
    "学習率の決定、収束性向上のために様々なアルゴリズムが利用されている\n",
    "  - Momentum\n",
    "  - AdaGrad\n",
    "  - Adadelta\n",
    "  - Adam\n",
    "- 確率的勾配降下法（SGD）\n",
    "<br>\n",
    "全サンプルの平均誤差ではなく、ランダムに抽出したサンプルの誤差を利用する\n",
    "<br>\n",
    "以下のメリットがある\n",
    "  - 計算コストの軽減\n",
    "  - 局所解に収束しにくい\n",
    "  - オンライン学習可能（都度パラメータを更新）\n",
    "  - メモリ利用が少ない\n",
    "- ミニバッチ勾配降下法\n",
    "<br>\n",
    "ランダムに分割したデータの集合（ミニバッチ）のサンプルの平均誤差を利用\n",
    "<br>\n",
    "深層学習では標準的に利用\n",
    "<br>\n",
    "確率的勾配降下法のメリットに加えて、並列化による計算資源の有効利用が可能"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 実装演習\n",
    "勾配降下法や確率的勾配降下法のPython実装を確認した\n",
    "<br>\n",
    "確率的勾配降下法では徐々に誤差が小さくなることが確認できた\n",
    "<br>\n",
    "ただしランダムに1つずつサンプルを抽出するために、誤差のブレが確認できた"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "深層学習ではミニバッチ勾配降下法がよく利用されるが、\n",
    "<br>\n",
    "ミニバッチのサイズをどうするかが性能に影響する。\n",
    "<br>\n",
    "複雑なネットワークではGPUの並列化機能を利用しないと学習に時間がかかるが、\n",
    "<br>\n",
    "ミニバッチのサイズを大きくするとGPUのメモリが足りず学習できない。\n",
    "<br>\n",
    "利用するGPUの性能、ネットワークの複雑さ、ミニバッチのサイズを合わせて検討する必要があり、\n",
    "<br>\n",
    "検討が難しい。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section4：誤差逆伝搬法"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- 数値微分\n",
    "<br>\n",
    "微小な値を入力に足した時の出力の差を計算して微分値に利用する\n",
    "<br>\n",
    "ニューラルネットワークの重みの微分には計算負荷が大きい\n",
    "- 誤差逆伝搬法\n",
    "<br>\n",
    "出力層から順に微分を計算し、その値を前の層に伝搬させることで各層の微分を計算\n",
    "<br>\n",
    "先に計算した結果を利用するため、効率的に計算可能\n",
    "<br>\n",
    "数値的な計算ではなく、解析的な計算\n",
    "<br>\n",
    "ニューラルネットワークや深層学習で標準的に利用される"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 実装演習\n",
    "誤差逆伝搬法のPython実装を確認した\n",
    "<br>\n",
    "出力層など先の層の微分計算の結果を利用して、前の層の微分計算を行う様子が確認できた\n",
    "<br>\n",
    "各層やユニットの微分計算を関数化など行うと、より恩恵が実感できるかもしれないと感じた\n",
    "<br>\n",
    "また、解析的な計算を行うため活性化関数の微分式を利用する必要がある"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "誤差逆伝搬法では偏微分の連鎖率を利用している。\n",
    "<br>\n",
    "単にネットワークを利用するためには深い理解は必要ないかもしれないが、\n",
    "<br>\n",
    "勾配消失問題などにはここが影響する。\n",
    "<br>\n",
    "学習がうまくいかない場合の対応などには中身の理解が必要と考えられるため、\n",
    "<br>\n",
    "中身の学習を行ったうえで利用したい。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 深層学習Day2\n",
    "## Section1：勾配消失問題"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- 概要\n",
    "<br>\n",
    "下位層に進むにつれて勾配が緩やかになる\n",
    "<br>\n",
    "更新でパラメータが変わらず、最適値に収束しない\n",
    "- 原因\n",
    "<br>\n",
    "活性化関数の1つであるシグモイド関数の微分が最大で0.25\n",
    "<br>\n",
    "下位層では逆伝搬の計算に何度も値を掛け合わせるため、勾配が小さくなる\n",
    "<br>\n",
    "- 活性化関数による解決法\n",
    "<br>\n",
    "RELU関数を利用する\n",
    "<br>\n",
    "RELU関数の微分は0か1なので、掛け合わせても値が小さくならない\n",
    "- 初期値による解決法\n",
    "<br>\n",
    "Xavierの初期値（シグモイド関数など）やHeの初期値（RELU関数など）を利用してネットワークの重みを初期化する\n",
    "<br>\n",
    "重みのバリエーションをつけつつ、前の層のノード数で重みを調整する\n",
    "<br>\n",
    "活性化関数の表現力を保ったまま、勾配の大きさを確保する\n",
    "- バッチ正規化による解決法\n",
    "<br>\n",
    "ミニバッチ単位によるデータの偏りを抑制する\n",
    "<br>\n",
    "メリット\n",
    "  - 学習速度を早くできる\n",
    "  - 過学習を抑えられる"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 実装演習\n",
    "<br>\n",
    "いくつかの方法で学習を行い比較した\n",
    "- シグモイド関数\n",
    "<br>\n",
    "基本的に学習が進まない\n",
    "<br>\n",
    "勾配消失問題が発生していると考えられる\n",
    "<br>\n",
    "ただし、学習の繰り返しを大きくするとステップ関数的にわずかに学習が進む\n",
    "<br>\n",
    "<img src=\"stage3_vanishing_sigmoid.png\">\n",
    "<br>\n",
    "- RELU\n",
    "<br>\n",
    "最初は学習が進まないが、その後学習がスムーズに進む\n",
    "<br>\n",
    "勾配消失問題が回避できていると考えられる\n",
    "<br>\n",
    "<img src=\"stage3_vanishing_relu.png\">\n",
    "<br>\n",
    "- シグモイド + Xavier\n",
    "<br>\n",
    "最初からスムーズに学習が進む\n",
    "<br>\n",
    "初期値の設定で勾配消失問題が回避できていると考えられる\n",
    "<br>\n",
    "<img src=\"stage3_vanishing_xavier.png\">\n",
    "<br>\n",
    "- RELU + He\n",
    "<br>\n",
    "最初からスムーズに学習が進む\n",
    "<br>\n",
    "最終的な精度もただのRELUより高くなっている\n",
    "<br>\n",
    "<img src=\"stage3_vanishing_he.png\">\n",
    "<br>\n",
    "- シグモイド + He\n",
    "<br>\n",
    "直観とは違い、学習が進んだ\n",
    "- RELU + Xavier\n",
    "<br>\n",
    "学習が進んだ\n",
    "- バッチ正規化\n",
    "<br>\n",
    "学習の精度が上がらない場合であっても、バッチ正規化を入れることで精度が上昇したことを確認した"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "RELU関数の利用や初期値の設定により、勾配消失問題を回避できることを学んだ。\n",
    "<br>\n",
    "近年の深層学習モデルは層が深くなっているため、これらの手法は重要だと考えられる。\n",
    "<br>\n",
    "一方で学習が進んでいないように見えても、ある時点で急に学習が進むような状況も見られた。\n",
    "<br>\n",
    "学習時の収束判定に影響するため、注意したい。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section2：学習率最適化手法"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- モメンタム\n",
    "<br>\n",
    "勾配降下法に完成を追加する\n",
    "<br>\n",
    "大域解になりやすい\n",
    "<br>\n",
    "収束までが早い\n",
    "- Adagrad\n",
    "<br>\n",
    "これまでの勾配に応じて学習率を徐々に小さくする\n",
    "<br>\n",
    "局所解ではないが勾配が0の箇所に収束してしまう（鞍点問題）\n",
    "- RMSProp\n",
    "<br>\n",
    "Adagradに比べて、過去の学習率を保持\n",
    "<br>\n",
    "大域解になりやすい\n",
    "<br>\n",
    "ハイパーパラメータの調整があまり必要ない\n",
    "- Adam\n",
    "<br>\n",
    "モメンタム + RMSProp"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 実装演習\n",
    "複数の最適化手法の比較を行った\n",
    "<br>\n",
    "最適化手法により学習が進むものと進まないものに分かれた\n",
    "<br>\n",
    "学習が進む手法であっても、ハイパーパラメータを変更するとうまくいかないことも分かった\n",
    "- SGD\n",
    "<br>\n",
    "学習が進まない\n",
    "- モメンタム\n",
    "<br>\n",
    "スムーズに学習できた\n",
    "- AdaGrad\n",
    "<br>\n",
    "スムーズに学習できた\n",
    "- RMSProp\n",
    "<br>\n",
    "スムーズに学習できた\n",
    "- Adam\n",
    "<br>\n",
    "スムーズに学習できた\n",
    "- Adam + 学習率0.1\n",
    "<br>\n",
    "学習が進まない"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "深層学習ライブラリの1つであるKerasには、\n",
    "<br>\n",
    "講義で紹介された以外にもいくつかの手法が利用できる。\n",
    "<br>\n",
    "研究段階ではどの最適化手法が良いかは結論が出ていないようである。\n",
    "- Adadelta\n",
    "- Adamax\n",
    "- Nadam\n",
    "\n",
    "また大きな勾配を制限するgradient clippingというテクニックもある。\n",
    "<br>\n",
    "こういった手法の利用やハイパーパラメータの調整など注意すべき点は多いように思う。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section3：過学習"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- 正則化\n",
    "<br>\n",
    "過学習を抑えるために、ニューラルネットの重みを制限する\n",
    "<br>\n",
    "L1、L2正則化による正則化項を誤差関数に加える\n",
    "<br>\n",
    "L1正則化のほうがスパースになりやすい\n",
    "- ドロップアウト\n",
    "<br>\n",
    "ランダムにノードを削除して学習する\n",
    "<br>\n",
    "異なるモデルを同時に学習させるような効果がある"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 実装演習\n",
    "複数の手法で過学習を抑えられるか確認した\n",
    "- 対策なし\n",
    "<br>\n",
    "訓練データの精度は1.0近くだが、テストデータでの精度は0.7程度\n",
    "<br>\n",
    "過学習の傾向が見える\n",
    "- L2正則化\n",
    "<br>\n",
    "ハイパーパラメータの調整も含めて、過学習の抑制はできなかった\n",
    "  - $\\lambda = 0.1$\n",
    "  <br>\n",
    "  訓練データの精度は下がったが、テストデータの精度は上がらず\n",
    "  - $\\lambda = 1.0$\n",
    "  <br>\n",
    "  訓練データもテストデータも大きく精度が下がった\n",
    "- L1正則化\n",
    "<br>\n",
    "ハイパーパラメータの調整も含めて、過学習の抑制はできなかった\n",
    "<br>\n",
    "L2正則化に比べて学習曲線のブレが大きい\n",
    "  - $\\lambda = 0.005$\n",
    "  <br>\n",
    "  テストデータの精度は上がらず\n",
    "  <br>\n",
    "  学習曲線のブレが大きい\n",
    "  - $\\lambda = 0.05$\n",
    "  <br>\n",
    "  訓練データもテストデータも大きく精度が下がった\n",
    "- ドロップアウト\n",
    "<br>\n",
    "ハイパーパラメータの調整を行ったが、過学習の抑制はできなかった\n",
    "<br>\n",
    "ドロップアウト率を上げると、収束までに時間がかかる傾向が見られた\n",
    "- ドロップアウト + L1正規化\n",
    "<br>\n",
    "ハイパーパラメータの調整を行ったが、過学習の抑制はできなかった"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### 考察\n",
    "過学習の根本的な原因は、モデルの複雑さに対してデータ数が足りないことであると考えられる。\n",
    "<br>\n",
    "その意味では、過学習の対策は２通りに分けられる。\n",
    "- モデルを単純にする\n",
    "  - 小さなモデルの利用\n",
    "  - 正則化\n",
    "  - ドロップアウト\n",
    "- データを増やす\n",
    "  - データの追加\n",
    "  - データ拡張"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section4：畳み込みニューラルネットワークの概念"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- 畳み込みニューラルネットワーク\n",
    "<br>\n",
    "画像など空間的・時間的につながりがあるものに使用される\n",
    "<br>\n",
    "畳み込み層・プーリング層などを持つ\n",
    "- 畳み込み層\n",
    "<br>\n",
    "フィルターとしての役割\n",
    "<br>\n",
    "次元のつながりを保ったまま、周囲の情報を集約する\n",
    "<br>\n",
    "パディングによってサイズが小さくならないようにできる\n",
    "<br>\n",
    "ストライドによって情報集約の間隔を調節する\n",
    "- プーリング層\n",
    "<br>\n",
    "畳み込み層と同時に利用される\n",
    "<br>\n",
    "畳み込み層と類似した演算を行うが、重みはなく最大・平均値のような計算を行う"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 実装演習\n",
    "畳み込み層やプーリング層などの実装を確認した\n",
    "<br>\n",
    "また中間層が少ない畳み込みニューラルネットワークの学習を行った\n",
    "<br>\n",
    "画像が得意であるため、MNISTデータセットに対する精度が高いのが確認できた\n",
    "<br>\n",
    "一方で学習にやや時間がかかるようにも見られた"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "近年の研究では画像データに対しても畳み込みではなく、\n",
    "<br>\n",
    "Transformerを利用したものが見られるが、\n",
    "<br>\n",
    "実用上はまだ画像などのデータに対しては\n",
    "<br>\n",
    "畳み込みニューラルネットワークを利用するシーンが多いと考えられる。\n",
    "<br>\n",
    "特に画像データに対する深層学習の需要は大きいため、\n",
    "<br>\n",
    "よく理解したい。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section5：最新のCNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### まとめ\n",
    "- AlexNet\n",
    "<br>\n",
    "5層の畳み込み層・プーリング層と3層の全結合層からなる\n",
    "<br>\n",
    "畳み込み層から全結合層に移るときには、1次元に並べ替えたのちに全結合層に通す\n",
    "<br>\n",
    "GlobalMaxPoolingやGlobalAveragePoolingでチャネルごとの最大・平均値を計算することもある\n",
    "<br>\n",
    "AlexNetでは出力にドロップアウトを利用している"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 実装演習\n",
    "畳み込み層が6層とこれまでに比べると深い畳み込みニューラルネットワークの学習を行った\n",
    "<br>\n",
    "非常に高い精度が出る一方で、学習に時間がかかる\n",
    "<br>\n",
    "深層学習には大きなリソースが必要であることを実感した"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 考察\n",
    "AlexNetは2012年に発表されたモデルであり、\n",
    "<br>\n",
    "その後さまざまな畳み込みニューラルネットワークが提案されている。\n",
    "<br>\n",
    "例えばKerasでも以下のようなネットワークが利用できる\n",
    "- VGG (2014)\n",
    "- ResNet (2015)\n",
    "- Inception (2015)\n",
    "- Xception (2016)\n",
    "- MobileNet (2017)\n",
    "\n",
    "中身や特徴を理解して利用することを心掛けたい。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}